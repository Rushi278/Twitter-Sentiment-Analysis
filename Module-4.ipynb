{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rushi\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that bummer you shoulda got david carr of thir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it not behaving at all mad why am here beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  that bummer you shoulda got david carr of thir...       0\n",
       "1  is upset that he can not update his facebook b...       0\n",
       "2  dived many times for the ball managed to save ...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it not behaving at all mad why am here beca...       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = 'clean_tweets.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1596064 entries, 0 to 1596063\n",
      "Data columns (total 2 columns):\n",
      "text      1596064 non-null object\n",
      "target    1596064 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Developing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1564142 entries with 50.01% negative, 49.99% positive\n",
      "Validation set has total 15961 entries with 49.72% negative, 50.28% positive\n",
      "Test set has total 15961 entries with 50.47% negative, 49.53% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 61.32%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive                7198                 827\n",
      "negative                5347                2589\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.33      0.46      7936\n",
      "           1       0.57      0.90      0.70      8025\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     15961\n",
      "   macro avg       0.67      0.61      0.58     15961\n",
      "weighted avg       0.67      0.61      0.58     15961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])\n",
    "print(\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion)\n",
    "print(\"-\"*80)\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_validation, tbpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print(\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR UNIGRAM WITHOUT STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 10000 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rushi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 50.28%\n",
      "accuracy score: 76.93%\n",
      "model is 26.65% more accurate than null accuracy\n",
      "train and test time: 62.17s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 20000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.23%\n",
      "model is 26.95% more accurate than null accuracy\n",
      "train and test time: 74.09s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 30000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.46%\n",
      "model is 27.19% more accurate than null accuracy\n",
      "train and test time: 88.28s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 40000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.51%\n",
      "model is 27.23% more accurate than null accuracy\n",
      "train and test time: 88.48s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 50000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.46%\n",
      "model is 27.18% more accurate than null accuracy\n",
      "train and test time: 102.40s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 60000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.46%\n",
      "model is 27.18% more accurate than null accuracy\n",
      "train and test time: 102.79s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 70000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.41%\n",
      "model is 27.13% more accurate than null accuracy\n",
      "train and test time: 96.63s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 80000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.42%\n",
      "model is 27.14% more accurate than null accuracy\n",
      "train and test time: 118.26s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 90000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.36%\n",
      "model is 27.08% more accurate than null accuracy\n",
      "train and test time: 121.18s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 100000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 77.39%\n",
      "model is 27.11% more accurate than null accuracy\n",
      "train and test time: 116.62s\n",
      "--------------------------------------------------------------------------------\n",
      "Wall time: 16min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
    "feature_result_wosw = nfeature_accuracy_checker(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR UNIGRAM WITH STOP WORDS\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Validation result for 10000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.29%\n",
      "model is 29.01% more accurate than null accuracy\n",
      "train and test time: 110.88s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 20000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.56%\n",
      "model is 29.28% more accurate than null accuracy\n",
      "train and test time: 151.92s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 30000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.75%\n",
      "model is 29.47% more accurate than null accuracy\n",
      "train and test time: 174.02s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 40000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.79%\n",
      "model is 29.51% more accurate than null accuracy\n",
      "train and test time: 186.07s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 50000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.84%\n",
      "model is 29.57% more accurate than null accuracy\n",
      "train and test time: 160.62s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 60000 features\n",
      "null accuracy: 50.28%\n",
      "accuracy score: 79.85%\n",
      "model is 29.57% more accurate than null accuracy\n",
      "train and test time: 209.56s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for 70000 features\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_ug = nfeature_accuracy_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv = 'term_freq_df.csv'\n",
    "term_freq_df = pd.read_csv(csv,index_col=0)\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "a = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))\n",
    "b = text.ENGLISH_STOP_WORDS\n",
    "set(a).issubset(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\")\n",
    "feature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "    confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
    "                         columns=['predicted_negative','predicted_positive'])\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"Confusion Matrix\\n\")\n",
    "    print(confusion)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Classification Report\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ug_cvec = CountVectorizer(max_features=80000)\n",
    "ug_pipeline = Pipeline([\n",
    "        ('vectorizer', ug_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(ug_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bg_cvec = CountVectorizer(max_features=70000,ngram_range=(1, 2))\n",
    "bg_pipeline = Pipeline([\n",
    "        ('vectorizer', bg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(bg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 3))\n",
    "tg_pipeline = Pipeline([\n",
    "        ('vectorizer', tg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
